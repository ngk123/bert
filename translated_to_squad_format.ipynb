{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "dev_file = '../squad/dev-v2.0.json'\n",
    "train_file = '../squad/train-v2.0.json'\n",
    "header_names = ['eq','ea','ec', 'es', 'ee', 'hq','ha','hc', 'hs', 'he']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_squad_dataset_skeleton(df, train_file, dev_file):\n",
    "    \"\"\"\n",
    "    Generates squad dataset skeleton with title and question sets (qids) which can be reused to create variants. \n",
    "    Questions not found in datasets have been grouped by their context and presented with synthetic title. \n",
    "    \n",
    "    Params:\n",
    "    df= dataframe of the translated SQUAD dataset\n",
    "    train_file= squad train dataset\n",
    "    dev_file = squad dev dataset\n",
    "    \n",
    "    Returns:\n",
    "    dataset_skeleton: Skeleton structure with title again list of question sets (for each context), where each question\n",
    "                     set contains question ids. \n",
    "    id_map: map of question ids to dataframe ids. \n",
    "    \"\"\"\n",
    "    parsed_v = None\n",
    "    parsed_t = None\n",
    "    with open(dev_file, 'r') as handle:\n",
    "        parsed_v = json.load(handle)\n",
    "    with open(train_file, 'r') as handle:\n",
    "        parsed_t = json.load(handle)\n",
    "    qid_to_dfid = {}\n",
    "    dataset_skeleton = {}\n",
    "\n",
    "    # Generate questions to dataframe id\n",
    "    qs_to_dfid = {}\n",
    "    for ind, row in df.iterrows():\n",
    "            qs_to_dfid[row['eq']] = ind\n",
    "\n",
    "    # Form dataset skeleton with title and paragraph lists containing related qids. Also form qid to dataframe id link.\n",
    "    for data in parsed_v['data']:\n",
    "        pars = []\n",
    "        \n",
    "        for par in data['paragraphs']:\n",
    "            qas = []\n",
    "            context = par['context']\n",
    "            for qa in par['qas']:\n",
    "                if not qa['is_impossible'] and qa['question'] in qs_to_dfid:\n",
    "                    df_id = qs_to_dfid[qa['question']]\n",
    "                    # There are multiple questions with same language in different contexts. \n",
    "                    if context != df.iloc[df_id]['ec']:\n",
    "                        continue\n",
    "                    qas.append(qa['id'])\n",
    "                    qid_to_dfid[qa['id']] = df_id\n",
    "            if qas:\n",
    "                pars.append(qas)\n",
    "        if pars:\n",
    "            dataset_skeleton[data['title']] = pars\n",
    "    for data in parsed_t['data']:\n",
    "        pars = []\n",
    "        for par in data['paragraphs']:\n",
    "            qas = []\n",
    "            context = par['context']\n",
    "            for qa in par['qas']:\n",
    "                if not qa['is_impossible'] and qa['question'] in qs_to_dfid:\n",
    "                    df_id = qs_to_dfid[qa['question']]\n",
    "                    # There are multiple questions with same language in different contexts. \n",
    "                    if context != df.iloc[df_id]['ec']:\n",
    "                        continue\n",
    "                    qas.append(qa['id'])\n",
    "                    qid_to_dfid[qa['id']] = df_id\n",
    "            if qas:\n",
    "                pars.append(qas)\n",
    "        if pars:\n",
    "            assert(data['title'] not in dataset_skeleton.keys())\n",
    "            dataset_skeleton[data['title']] = pars\n",
    "    # Add those questions not found in datasets. \n",
    "    marked = set(qid_to_dfid.values())\n",
    "    ctx_to_id = {}\n",
    "    for ind, row in df.iterrows():\n",
    "        if ind not in marked:\n",
    "            if row['ec'] not in ctx_to_id:\n",
    "                ctx_to_id[row['ec']] = [ind]\n",
    "            else:\n",
    "                ctx_to_id[row['ec']].append(ind)\n",
    "    for index, (_, ids) in enumerate(ctx_to_id.items()):\n",
    "        title = 'NO_TITLE_'+str(index)\n",
    "        qs = []\n",
    "        for i in ids:\n",
    "            qid = title + '_Q_'+str(i)\n",
    "            qid_to_dfid[qid] = i\n",
    "            qs.append(qid)\n",
    "        dataset_skeleton[title] = [qs]\n",
    "    return dataset_skeleton, qid_to_dfid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_squad_dataset_variant(skeleton, df, q_type='e', a_type='e'):\n",
    "    \"\"\"\n",
    "    Takes the skeleton structure (dataset_skeleton and id map) and generates the synthetic dataset json based on the \n",
    "    question and answer type passed. h for hindi and e for english. Variants can be on question type and context type. \n",
    "    \n",
    "    Params:\n",
    "    skeleton: Pair passed from generate_squad_dataset_skeleton. \n",
    "    df: dataframe of the translated SQUAD dataset\n",
    "    q_type: language of question. h for hindi and e for english\n",
    "    a_type: language of answer and context. h for hindi and e for english\n",
    "    \n",
    "    Returns: \n",
    "    Synthentic SQuAD dataset dict. Should be dumped as json by user. \n",
    "    \n",
    "    \"\"\"\n",
    "    types = ['e', 'h']\n",
    "    if q_type not in types or a_type not in types:\n",
    "        raise ValueError(\"types should be e or h\")\n",
    "    data = []\n",
    "    id_map = skeleton[1]\n",
    "    skeleton = skeleton[0]\n",
    "    for title, qsets in skeleton.items():\n",
    "        paragraphs = []\n",
    "        for qset in qsets:\n",
    "            qas = []\n",
    "            context = None\n",
    "            for q in qset:\n",
    "                context = df.iloc[id_map[q]]['ec' if a_type=='e' else 'hc']\n",
    "                answer = df.iloc[id_map[q]]['ea' if a_type=='e' else 'ha']\n",
    "                if context.find(answer) == -1:\n",
    "                    print(answer, context)\n",
    "                    assert(False)\n",
    "                qas.append({\n",
    "                    \"id\": q,\n",
    "                    \"answers\": [\n",
    "                        {\n",
    "                            \"text\": answer,\n",
    "                            \"answer_start\": context.find(answer)\n",
    "                        }\n",
    "                    ],\n",
    "                    \"is_impossible\": True if not answer else False,\n",
    "                    \"question\": df.iloc[id_map[q]]['eq' if q_type=='e' else 'hq']\n",
    "                })\n",
    "            paragraphs.append({\n",
    "                'context': context,\n",
    "                'qas': qas\n",
    "            })\n",
    "        data.append({\n",
    "            'paragraphs': paragraphs,\n",
    "            'title': title\n",
    "        }) \n",
    "        \n",
    "    return {\n",
    "        'data' : data,\n",
    "        'version' : 'v2.0'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanse unneeded whitespace in answers \n",
    "def cleanse_answer_eng(text):\n",
    "    new_text = ''\n",
    "    for idx, ch in enumerate(text):\n",
    "        new_text= new_text+ch \n",
    "        # phrases that don't no space before\n",
    "        no_space_before = [',', '.', '%', '°', 'FM', '/','-', '–', '♠', '×', 'n\\'t', ';','+',':', '\\'ve','´']\n",
    "        # phrases that don't no space after\n",
    "        no_space_after = ['#', '$', '°', '/','-', '–', '♠', '×','+','´']\n",
    "        # phrases to replace with\n",
    "        replace_with = {'--':'–', '\\'\\'':'\"', '\\u00a0':' ', '``':'\"', '\\'`':'\"', '`':'\\'',\n",
    "                       'can not':'cannot', '. nf':'.nf', ' \\'s': '\\'s', '’':'\\'', '..':'.', 'a \" tree':'a \"tree',\n",
    "                        's \\'':'s\\'', '\\' aesthetic \\'':'\\'aesthetic\\'', '. mp3':'.mp3', '\" a':'\"a'}\n",
    "        for ele in no_space_before:\n",
    "            if new_text.endswith(ele) and len(new_text)>len(ele) and new_text[-len(ele)-1]==' ':\n",
    "                new_text = new_text[:-len(ele)-1] + new_text[-len(ele):]\n",
    "                break\n",
    "        if new_text[-1]==' ':\n",
    "            for ele in no_space_after:\n",
    "                if new_text[:-1].endswith(ele):\n",
    "                    new_text = new_text[:-1]\n",
    "                    break\n",
    "        for ele, rele in replace_with.items():\n",
    "            if new_text.endswith(ele):\n",
    "                new_text = new_text[:-len(ele)] + rele\n",
    "        #Resolve degree issues - temperature after, degree before. \n",
    "        if len(new_text) > 2 and new_text[-2] == '°':\n",
    "            if new_text[-1] in ['C', 'F']:\n",
    "                new_text = new_text[:-2] + ' ' + new_text[-2:]\n",
    "            else:\n",
    "                new_text = new_text[:-1] + ' ' + new_text[-1:]\n",
    "        if ch.isdigit() and len(new_text)>=3 and new_text[-2].isspace() and new_text[-3].isdigit():\n",
    "            new_text = new_text[:-2]+new_text[-1]\n",
    "    dont_end_with = [ '\\'', '\"', '.', ',']\n",
    "    for ele in dont_end_with:\n",
    "        if new_text.endswith(ele):\n",
    "            new_text = new_text[:-len(ele)]\n",
    "            break\n",
    "    return new_text.strip()\n",
    "\n",
    "def cleanse_answer_hin(text):\n",
    "    new_text = ''\n",
    "    no_space_before = [',', '.', '%', '°', 'FM', '/','-', '–', '♠', '×', 'n\\'t', ';','+', ':']\n",
    "    no_space_after = ['#', '$', '°', '/','-', '–', '♠', '×']\n",
    "    replace_with = {', और':' और', '--':'–', '\\'\\'':'\"', '\\u00a0':' ', '``':'\"', '\\'`':'\"', '`':'\\'', \n",
    "                    ', या':' या', 'अक्टूबर ': 'अक्टूबर, ', 'दिसंबर ': 'दिसंबर, ', 'मई ': 'मई, ', 'जनवरी ': 'जनवरी, ',\n",
    "                    'मार्च ': 'मार्च, ', 'नवंबर ': 'नवंबर, ', 'अगस्त ': 'अगस्त, ', 'सितंबर ': 'सितंबर, ', 'अप्रैल ': 'अप्रैल, ',\n",
    "                   'जुलाई ': 'जुलाई, ', 'जून ': 'जून, ', 'जे।': 'जे', 'एडी':'ए.डी.', 'ए.डी':'ए.डी.', 'एस।': 'एस', 'यू.एस.':'यूएस', \n",
    "                    'आर।':'आर', 'डॉ।':'डॉ.', 'डॉ ':'डॉ. ', '..':'.', 'सी।':'सी', 'एल।':'एल', 'जी।':'जी'}\n",
    "    for idx, ch in enumerate(text):\n",
    "        new_text = new_text + ch\n",
    "        for ele in no_space_before:\n",
    "            if new_text.endswith(ele) and len(new_text)>len(ele) and new_text[-len(ele)-1]==' ':\n",
    "                new_text = new_text[:-len(ele)-1] + new_text[-len(ele):]\n",
    "                break\n",
    "        if new_text[-1]==' ':\n",
    "            for ele in no_space_after:\n",
    "                if new_text[:-1].endswith(ele):\n",
    "                    new_text = new_text[:-1]\n",
    "                    break\n",
    "        for ele, rele in replace_with.items():\n",
    "            if new_text.endswith(ele):\n",
    "                new_text = new_text[:-len(ele)] + rele\n",
    "        if ch == ',' and len(new_text)>1 and new_text[-2].isdigit():\n",
    "            new_text = new_text[:-1]\n",
    "        if ch.isdigit() and len(new_text)>=3 and new_text[-2].isspace() and new_text[-3].isdigit():\n",
    "            new_text = new_text[:-2]+new_text[-1]\n",
    "    dont_end_with = [',', '.', '\"', '।', '\\'']\n",
    "    for ele in dont_end_with:\n",
    "            if new_text.endswith(ele):\n",
    "                new_text = new_text[:-len(ele)]\n",
    "                break\n",
    "    return new_text.strip()\n",
    "    \n",
    "def santize_answers_in_dataset(df):\n",
    "    for ind, row in df.iterrows():\n",
    "            df.iloc[ind,1] = cleanse_answer_eng(df.iloc[ind]['ea'])\n",
    "            df.iloc[ind,2] = cleanse_answer_eng(df.iloc[ind]['ec'])\n",
    "            if df.iloc[ind]['ec'].find(df.iloc[ind]['ea']) == -1:\n",
    "                print(str(ind),'='*5 ,df.iloc[ind]['ea'], '='*5 ,df.iloc[ind]['ec'])\n",
    "                assert(False)\n",
    "            df.iloc[ind,6] = cleanse_answer_hin(df.iloc[ind]['ha'])\n",
    "            df.iloc[ind,7] = cleanse_answer_hin(df.iloc[ind]['hc'])\n",
    "            if df.iloc[ind]['hc'].find(df.iloc[ind]['ha']) == -1:\n",
    "                print(str(ind),'='*5 , df.iloc[ind]['ha'], '='*5 ,df.iloc[ind]['hc'])\n",
    "                assert(False)\n",
    "    return df\n",
    "\n",
    "#sanity test\n",
    "assert(cleanse_answer_eng('hello , hi . ') =='hello, hi.')\n",
    "assert(cleanse_answer_eng('9 0') =='90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train - incomplete\n",
    "df_train = pd.read_csv(\"./translated/train_data.txt\", sep='\\t', encoding='UTF-8', header=None, index_col=False,\n",
    "                 names=header_names) \n",
    "df_train = santize_answers_in_dataset(df_train)\n",
    "df_train.to_csv(\"./translated/train_data_t.txt\", sep='\\t', header=None, index=False)\n",
    "df_train = pd.read_csv(\"./translated/train_data_t.txt\", sep='\\t', encoding='UTF-8', header=None, index_col=False,\n",
    "                 names=header_names)\n",
    "skeleton_train = generate_squad_dataset_skeleton(df_train, train_file, dev_file)\n",
    "with open('./translated/train_data_eq_ec.json', 'w') as fp:\n",
    "    json.dump(generate_squad_dataset_variant(skeleton_train, df_train, q_type='e', a_type='e'), fp)\n",
    "with open('./translated/train_data_eq_hc.json', 'w') as fp:\n",
    "    json.dump(generate_squad_dataset_variant(skeleton_train, df_train, q_type='e', a_type='h'), fp)\n",
    "with open('./translated/train_data_hq_ec.json', 'w') as fp:\n",
    "    json.dump(generate_squad_dataset_variant(skeleton_train, df_train, q_type='h', a_type='e'), fp)\n",
    "with open('./translated/train_data_hq_hc.json', 'w') as fp:\n",
    "    json.dump(generate_squad_dataset_variant(skeleton_train, df_train, q_type='h', a_type='h'), fp)\n",
    "df_train.to_csv(\"./translated/train_data_t.txt\", sep='\\t', header=None, index=False)\n",
    "os.remove(\"./translated/train_data_t.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def sanity_check(dataset_file):\n",
    "    dataset = None\n",
    "    with open(dataset_file, 'r', encoding='utf-8') as fp:\n",
    "        dataset = json.load(fp)\n",
    "    datas = []\n",
    "    for data in dataset['data']:\n",
    "        paragraphs = []\n",
    "        for par in data['paragraphs']:\n",
    "            qas = []\n",
    "            context = par['context']\n",
    "            for qa in par['qas']:\n",
    "                new_qa = copy.deepcopy(qa)\n",
    "                answer = qa['answers'][0]['text']\n",
    "                start = qa['answers'][0]['answer_start']\n",
    "                if not context[start:].startswith(answer):\n",
    "                    if context[start:].find(answer)==-1:\n",
    "                        print('here',qa['question'], answer, context.find(answer), context)\n",
    "                        assert(False)\n",
    "                    else:\n",
    "                        print(answer, start, context[start:].find(answer))\n",
    "                        new_qa['answers'][0]['answer_start'] = start+context[start:].find(answer)\n",
    "                qas.append(new_qa)\n",
    "            par['qas'] = qas\n",
    "            paragraphs.append(par)\n",
    "        data['paragraphs'] = paragraphs\n",
    "        datas.append(data)\n",
    "    dataset['data'] = datas\n",
    "    with open(dataset_file, 'w', encoding='utf-8') as fp:\n",
    "        json.dump(dataset, fp)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check(\"./translated/train_data_eq_ec.json\")\n",
    "sanity_check(\"./translated/train_data_eq_hc.json\")\n",
    "sanity_check(\"./translated/train_data_hq_ec.json\")\n",
    "sanity_check(\"./translated/train_data_hq_hc.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "df_test = pd.read_csv(\"./translated/test_data.txt\", sep='\\t', encoding='UTF-8', header=None, index_col=False,\n",
    "                 names=header_names) \n",
    "df_test = santize_answers_in_dataset(df_test)\n",
    "skeleton_test = generate_squad_dataset_skeleton(df_test, train_file, dev_file)\n",
    "with open('./translated/test_data_eq_ec.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(generate_squad_dataset_variant(skeleton_test, df_test, q_type='e', a_type='e'), fp)\n",
    "with open('./translated/test_data_eq_hc.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(generate_squad_dataset_variant(skeleton_test, df_test, q_type='e', a_type='h'), fp)\n",
    "with open('./translated/test_data_hq_ec.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(generate_squad_dataset_variant(skeleton_test, df_test, q_type='h', a_type='e'), fp)\n",
    "with open('./translated/test_data_hq_hc.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(generate_squad_dataset_variant(skeleton_test, df_test, q_type='h', a_type='h'), fp)\n",
    "df_test.to_csv(\"./translated/test_data.txt\", sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check(\"./translated/test_data_eq_ec.json\")\n",
    "sanity_check(\"./translated/test_data_eq_hc.json\")\n",
    "sanity_check(\"./translated/test_data_hq_ec.json\")\n",
    "sanity_check(\"./translated/test_data_hq_hc.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Val\n",
    "df_val = pd.read_csv(\"./translated/val_data.txt\", sep='\\t', encoding='UTF-8', header=None, index_col=False,\n",
    "                 names=header_names) \n",
    "df_val = santize_answers_in_dataset(df_val)\n",
    "df_val.to_csv(\"./translated/val_data_t.txt\", sep='\\t', header=None, index=False)\n",
    "df_val = pd.read_csv(\"./translated/val_data_t.txt\", sep='\\t', encoding='UTF-8', header=None, index_col=False,\n",
    "                 names=header_names)\n",
    "skeleton_val = generate_squad_dataset_skeleton(df_val, train_file, dev_file)\n",
    "with open('./translated/val_data_eq_ec.json', 'w') as fp:\n",
    "    json.dump(generate_squad_dataset_variant(skeleton_val, df_val, q_type='e', a_type='e'), fp)\n",
    "with open('./translated/val_data_eq_hc.json', 'w') as fp:\n",
    "    json.dump(generate_squad_dataset_variant(skeleton_val, df_val, q_type='e', a_type='h'), fp)\n",
    "with open('./translated/val_data_hq_ec.json', 'w') as fp:\n",
    "    json.dump(generate_squad_dataset_variant(skeleton_val, df_val, q_type='h', a_type='e'), fp)\n",
    "with open('./translated/val_data_hq_hc.json', 'w') as fp:\n",
    "    json.dump(generate_squad_dataset_variant(skeleton_val, df_val, q_type='h', a_type='h'), fp)\n",
    "df_val.to_csv(\"./translated/val_data_t.txt\", sep='\\t', header=None, index=False)\n",
    "os.remove(\"./translated/val_data_t.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check(\"./translated/val_data_eq_ec.json\")\n",
    "sanity_check(\"./translated/val_data_eq_hc.json\")\n",
    "sanity_check(\"./translated/val_data_hq_ec.json\")\n",
    "sanity_check(\"./translated/val_data_hq_hc.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
